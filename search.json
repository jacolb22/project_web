[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "README",
    "section": "",
    "text": "Include an abstract-style summary of your project here which includes the research area, rationale, aims, and findings.",
    "crumbs": [
      "README"
    ]
  },
  {
    "objectID": "index.html#project-title",
    "href": "index.html#project-title",
    "title": "README",
    "section": "",
    "text": "Include an abstract-style summary of your project here which includes the research area, rationale, aims, and findings.",
    "crumbs": [
      "README"
    ]
  },
  {
    "objectID": "index.html#reproduce",
    "href": "index.html#reproduce",
    "title": "README",
    "section": "Reproduce",
    "text": "Reproduce\n\nInclude a step-by-step guide to reproduce your project here. This should include a list of dependencies and instructions on how to run the project.",
    "crumbs": [
      "README"
    ]
  },
  {
    "objectID": "reports/slides/example.html#references",
    "href": "reports/slides/example.html#references",
    "title": "My Presentation",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis Vaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr."
  },
  {
    "objectID": "reports/prospectus.html",
    "href": "reports/prospectus.html",
    "title": "Prospectus",
    "section": "",
    "text": "This section introduces the topic of the prospectus, its relevance, and specific research question(s) or hypothesis(es) that will be addressed.\n\n\n\n\nThe aim of the first study: “Intensificatory Tautology in the History of English: A Corpus-based Study” by Victorina González-Díaz is exploratory in nature, and concerns the development and establishment of “intensificatory tautology” which is simply intensifying the current meaning of a phrase or sentence without adding any more meaning. It focuses specifically on size-adjective clusters (phrases like “great big” and “little tiny”) in the history of the English language. The findings of this study indicate that size-adjective clusters emerged in Late Middle English after a functional-structural reorganization of noun phrases. This became more productive in Early Modern English, and expanded in Present-Day English, especially in informal and spoken settings. This study used many corpora including: the PENN collection, the ARCHER corpus, the British National Corpus (BNC), the Early English Books Online (EEBO) corpus, Mandeville’s travels, and Celia Fiennes’ travelogue. The analyzing methodology was corpus-based, and used part of speech-tagged text as well as a general search for two consecutive adjectives to prune the data to a manageable level, and the rest was manually sorted through. The study also used statistical methods such as MI scores and T-tests. There is a research gap, even mentioned in the study itself, regarding how collocation interacts with word-formation processes, especially in this context. This study explains the what and where, but being exploratory in nature, it does not mention the why.\n\n\n\nThe aim of the study “Internal and External Dynamics in Language: Evidence from Verb Regularity in a Historical Corpus of English” is to investigate the relationship between internal and external factors affecting the evolution of rules and exceptions in language, specifically focusing on the how regular the past tense verbs in American English are. The main findings of the study indicate that although the language has a continuous influx of new verbs, the overall amount of irregularity in the language remains relatively constant over time. The study used the Corpus of Historical American English (CoHA), which contains a balanced variety of written genres and more than 330 million words from the 1830–1989 period, each tagged for part of speech. The analysis approach involved a quantitative examination of factors contributing to language change, focusing on the effects of verb birth and death, as well as the regularizing or irregularizing of existing words. The study also used statistical methods to analyze the data, including the proportion of irregularity as the number of irregular past tense tokens over the total number of past tense tokens, and frequency-rank plots for different verb types in all decades. There is a research gap in that the study does not mention how certain words may change regularity over time, and whether certain words are more “volatile” than others.\n\n\n\nThe aim of the study “Outdooring the Historical Corpus of English in Ghana” by Thorsten Brato is to compile a historical corpus of Ghanaian English (GhE) and to illustrate in real time how GhE has developed over its nativization phase. The main findings of the study indicate that GhE has been influenced by factors including: demographics, migration patterns, and language attitudes, leading to deviations from the developmental path suggested in the Dynamic Mode (I will not go into depth about the Dynamic Mode here, for more information, see the original study) . The study used the Historical Corpus of English in Ghana (HiCE Ghana), a 600,000-word corpus of GhE from the period 1966–1975, and compared it with the written-printed sections of the Ghanaian component of ICE (another corpus of GhE) to uncover how GhE has changed over the last two generations. The analysis approach involved a real-time comparison of empirical data and proposed adaptations where necessary. The study also used statistical methods to analyze the data, such as comparing data from ICE corpora of varieties which have progressed to different phases and taking an apparent-time approach to test developments within one variety. The findings contribute to a nuanced understanding of the linguistic landscape in Ghana. There is a research gap in that the study does not include other postcolonial Englishes, which would help to compare the GhE findings against other similar Englishes, to see if they are consistent to other observed changes.\n\n\n\nThe aim of the study “Sociolinguistic Typology Meets Historical Corpus Linguistics” is to showcase how the integration of historical corpora into research methodologies can enrich our understanding of sociolinguistic typology, as highlighted by three case studies exploring potential morphosyntactic simplifications. These case studies delve into the loss of number concordance in English, changes in Latin American Spanish’s null-subject system(s), and reductions in the case system in Balkan Slavic, revealing that historical corpus evidence can effectively contribute to addressing questions within sociolinguistic typology, facilitating the testing of theoretical predictions and the identification of new influencing factors. The dataset utilized in the study comprises diverse texts ranging from Middle English horse treatises to Bulgarian saints’ lives to Afro-Colombian poetry, totaling over 50 texts sourced from seven countries with AHLA-speaking communities, alongside Spain as a control. The study makes use of various historical corpora such as CDE, CORDE/CDH, and CORDIAM, in addition to freely available texts from databases like Cervantes Virtual, BDH, and DLOC. Methodologically, the research adopts the “variationist” historical corpus method, which assesses the prevalence of different linguistic variants across different times and places. Statistical methods are employed to evaluate predictions, comparing texts from various times and geographical contexts. The study also addresses methodological challenges related to compiling and annotating corpora, particularly in the context of null subjects, employing techniques such as linear regression models and corpus balancing to ensure homogeneity. There is a research gap regarding the “why” behind this study, as these case studies are interesting, but this study simply shows that corpus linguistics can be used in sociolinguistic contexts. It does not show why the results are the way they are, or really what it means for each case study individually.\n\n\n\nThe aim of the study “The night before beg’d ye queens’s pardon and his brother’s: the apostrophe in the history of English” by Javier Calle-Martín and Marta Pacheco-Franco is to investigate the standardization of the apostrophe in the English orthographic system in the period 1600–1900. The study pursues the objectives of studying the use and omission of the apostrophe in the expression of the past tense, the genitive case, and the nominative plural in the period, assessing the relationship between the three uses and their likely connections, and evaluating the likely participation of grammarians in the adoption and the rejection of each of these phenomena in English. The main findings of the study indicate that the systematic adoption of the apostrophe in the English punctuation is a late sixteenth-century development, and the symbol quickly gained in popularity and acquired a variety of functions that included the past tense (-’d), the genitive case (-’s), and the nominative plural (-’s). The study also shows that the possessive apostrophe was introduced earlier than accounted for in the literature and had spread and overcome its counterpart by the early eighteenth century. The study used the A Representative Corpus of Historical English Registers (ARCHER 3.2) and the Early English Books Online corpus (EEBO) as sources of evidence. The methodology involved the extraction of data from these corpora, and the analysis of a total of 166,550 instances from ARCHER and 836,805 instances from EEBO. The study also used statistical methods to compare frequencies over time and across genres, and to normalize the data to a common base of words. The research gap lies in that the study does not note if those early uses of apostrophes had different counterparts before the introduction of the apostrophe, and if those uses have a history of being linguistically volatile. According to the way the study has presented the information and background, those uses would have had a minimum of two changes in 300 years, so it would call into question how often those uses change, and even if there are similar parallels in other languages.\n\n\n\nThe research gap that I have identified regards the scope of the research in question. I believe that many of these studies explore deeply into a very specific area. This allows these studies to crawl through data that can explain the intricacies of the phenomena at hand, but does not allow the studies to see the bigger picture. More data and more varied data would allow these studies or another study to compare phenomena between languages and contexts. This could lead to new understanding of language development in context, for example. My research question, thus, is: do postcolonial languages develop new words at the same rate, or does the country’s independence either slow or hasten the divergence of postcolonial language? The objectives of this research would be to answer whether or not postcolonial languages are affected by a country’s independence. The expected impact of this research is to expand upon the current research in this area, and to make connections between different languages and their divergence through time. Perhaps concrete stages can be developed to give a general guide about how postcolonial languages develop.\n\n\n\nI learned how to identify a research gap, and how to craft a research question around that gap. I found it challenging to read all of the studies and analyze them, however just due to the amount of information that was new to me. I didn’t consult any resources for this process, as all of the analyzing must be done by me. I do have a few questions that pertain to my research question: is it detailed enough, or too much? Is it supposed to be more connected with all 5 studies, or is it alright that I just chose one to base it off of? I will need reassurance on these questions to move forward.",
    "crumbs": [
      "Reports",
      "Prospectus"
    ]
  },
  {
    "objectID": "reports/prospectus.html#literature-review",
    "href": "reports/prospectus.html#literature-review",
    "title": "Prospectus",
    "section": "",
    "text": "The aim of the first study: “Intensificatory Tautology in the History of English: A Corpus-based Study” by Victorina González-Díaz is exploratory in nature, and concerns the development and establishment of “intensificatory tautology” which is simply intensifying the current meaning of a phrase or sentence without adding any more meaning. It focuses specifically on size-adjective clusters (phrases like “great big” and “little tiny”) in the history of the English language. The findings of this study indicate that size-adjective clusters emerged in Late Middle English after a functional-structural reorganization of noun phrases. This became more productive in Early Modern English, and expanded in Present-Day English, especially in informal and spoken settings. This study used many corpora including: the PENN collection, the ARCHER corpus, the British National Corpus (BNC), the Early English Books Online (EEBO) corpus, Mandeville’s travels, and Celia Fiennes’ travelogue. The analyzing methodology was corpus-based, and used part of speech-tagged text as well as a general search for two consecutive adjectives to prune the data to a manageable level, and the rest was manually sorted through. The study also used statistical methods such as MI scores and T-tests. There is a research gap, even mentioned in the study itself, regarding how collocation interacts with word-formation processes, especially in this context. This study explains the what and where, but being exploratory in nature, it does not mention the why.\n\n\n\nThe aim of the study “Internal and External Dynamics in Language: Evidence from Verb Regularity in a Historical Corpus of English” is to investigate the relationship between internal and external factors affecting the evolution of rules and exceptions in language, specifically focusing on the how regular the past tense verbs in American English are. The main findings of the study indicate that although the language has a continuous influx of new verbs, the overall amount of irregularity in the language remains relatively constant over time. The study used the Corpus of Historical American English (CoHA), which contains a balanced variety of written genres and more than 330 million words from the 1830–1989 period, each tagged for part of speech. The analysis approach involved a quantitative examination of factors contributing to language change, focusing on the effects of verb birth and death, as well as the regularizing or irregularizing of existing words. The study also used statistical methods to analyze the data, including the proportion of irregularity as the number of irregular past tense tokens over the total number of past tense tokens, and frequency-rank plots for different verb types in all decades. There is a research gap in that the study does not mention how certain words may change regularity over time, and whether certain words are more “volatile” than others.\n\n\n\nThe aim of the study “Outdooring the Historical Corpus of English in Ghana” by Thorsten Brato is to compile a historical corpus of Ghanaian English (GhE) and to illustrate in real time how GhE has developed over its nativization phase. The main findings of the study indicate that GhE has been influenced by factors including: demographics, migration patterns, and language attitudes, leading to deviations from the developmental path suggested in the Dynamic Mode (I will not go into depth about the Dynamic Mode here, for more information, see the original study) . The study used the Historical Corpus of English in Ghana (HiCE Ghana), a 600,000-word corpus of GhE from the period 1966–1975, and compared it with the written-printed sections of the Ghanaian component of ICE (another corpus of GhE) to uncover how GhE has changed over the last two generations. The analysis approach involved a real-time comparison of empirical data and proposed adaptations where necessary. The study also used statistical methods to analyze the data, such as comparing data from ICE corpora of varieties which have progressed to different phases and taking an apparent-time approach to test developments within one variety. The findings contribute to a nuanced understanding of the linguistic landscape in Ghana. There is a research gap in that the study does not include other postcolonial Englishes, which would help to compare the GhE findings against other similar Englishes, to see if they are consistent to other observed changes.\n\n\n\nThe aim of the study “Sociolinguistic Typology Meets Historical Corpus Linguistics” is to showcase how the integration of historical corpora into research methodologies can enrich our understanding of sociolinguistic typology, as highlighted by three case studies exploring potential morphosyntactic simplifications. These case studies delve into the loss of number concordance in English, changes in Latin American Spanish’s null-subject system(s), and reductions in the case system in Balkan Slavic, revealing that historical corpus evidence can effectively contribute to addressing questions within sociolinguistic typology, facilitating the testing of theoretical predictions and the identification of new influencing factors. The dataset utilized in the study comprises diverse texts ranging from Middle English horse treatises to Bulgarian saints’ lives to Afro-Colombian poetry, totaling over 50 texts sourced from seven countries with AHLA-speaking communities, alongside Spain as a control. The study makes use of various historical corpora such as CDE, CORDE/CDH, and CORDIAM, in addition to freely available texts from databases like Cervantes Virtual, BDH, and DLOC. Methodologically, the research adopts the “variationist” historical corpus method, which assesses the prevalence of different linguistic variants across different times and places. Statistical methods are employed to evaluate predictions, comparing texts from various times and geographical contexts. The study also addresses methodological challenges related to compiling and annotating corpora, particularly in the context of null subjects, employing techniques such as linear regression models and corpus balancing to ensure homogeneity. There is a research gap regarding the “why” behind this study, as these case studies are interesting, but this study simply shows that corpus linguistics can be used in sociolinguistic contexts. It does not show why the results are the way they are, or really what it means for each case study individually.\n\n\n\nThe aim of the study “The night before beg’d ye queens’s pardon and his brother’s: the apostrophe in the history of English” by Javier Calle-Martín and Marta Pacheco-Franco is to investigate the standardization of the apostrophe in the English orthographic system in the period 1600–1900. The study pursues the objectives of studying the use and omission of the apostrophe in the expression of the past tense, the genitive case, and the nominative plural in the period, assessing the relationship between the three uses and their likely connections, and evaluating the likely participation of grammarians in the adoption and the rejection of each of these phenomena in English. The main findings of the study indicate that the systematic adoption of the apostrophe in the English punctuation is a late sixteenth-century development, and the symbol quickly gained in popularity and acquired a variety of functions that included the past tense (-’d), the genitive case (-’s), and the nominative plural (-’s). The study also shows that the possessive apostrophe was introduced earlier than accounted for in the literature and had spread and overcome its counterpart by the early eighteenth century. The study used the A Representative Corpus of Historical English Registers (ARCHER 3.2) and the Early English Books Online corpus (EEBO) as sources of evidence. The methodology involved the extraction of data from these corpora, and the analysis of a total of 166,550 instances from ARCHER and 836,805 instances from EEBO. The study also used statistical methods to compare frequencies over time and across genres, and to normalize the data to a common base of words. The research gap lies in that the study does not note if those early uses of apostrophes had different counterparts before the introduction of the apostrophe, and if those uses have a history of being linguistically volatile. According to the way the study has presented the information and background, those uses would have had a minimum of two changes in 300 years, so it would call into question how often those uses change, and even if there are similar parallels in other languages.\n\n\n\nThe research gap that I have identified regards the scope of the research in question. I believe that many of these studies explore deeply into a very specific area. This allows these studies to crawl through data that can explain the intricacies of the phenomena at hand, but does not allow the studies to see the bigger picture. More data and more varied data would allow these studies or another study to compare phenomena between languages and contexts. This could lead to new understanding of language development in context, for example. My research question, thus, is: do postcolonial languages develop new words at the same rate, or does the country’s independence either slow or hasten the divergence of postcolonial language? The objectives of this research would be to answer whether or not postcolonial languages are affected by a country’s independence. The expected impact of this research is to expand upon the current research in this area, and to make connections between different languages and their divergence through time. Perhaps concrete stages can be developed to give a general guide about how postcolonial languages develop.\n\n\n\nI learned how to identify a research gap, and how to craft a research question around that gap. I found it challenging to read all of the studies and analyze them, however just due to the amount of information that was new to me. I didn’t consult any resources for this process, as all of the analyzing must be done by me. I do have a few questions that pertain to my research question: is it detailed enough, or too much? Is it supposed to be more connected with all 5 studies, or is it alright that I just chose one to base it off of? I will need reassurance on these questions to move forward.",
    "crumbs": [
      "Reports",
      "Prospectus"
    ]
  },
  {
    "objectID": "reports/prospectus.html#data-preparation",
    "href": "reports/prospectus.html#data-preparation",
    "title": "Prospectus",
    "section": "Data preparation",
    "text": "Data preparation\n\nIdeal Data\nThe ideal data for the aforementioned research question needs to have both the depth to faithfully represent the new words being written in postcolonial Englishes, and have the width to be able to compare the statistics gleaned from at least a few different languages. The dataset will likely have to comprise multiple other datasets, which will likely be hard to find. I anticipate having to merge a few datasets together, making sure to retain as much information as possible, especially year and location, as these will be most important to this analysis. Speaking of the individual data, I would hope it is “tidy”, with each row representing a data point and each column telling information about the data point. I hope that I am able to find data that is of written and oral modalities, and although this project is focused on written modalities moreso, if someone in the future wanted to expand my research to oral modalities, I would hope the data I can find will support that research. The ideal data would have the words already tokenized, with their definitions in American English, along with their part of speech, some measure of their popularity, the year they rose to prominence, their modality, their country of origin, and perhaps the origin of the word in question, including the gender and age of who wrote it, and the document ID from which it came. The ideal dataset would be free to use for my purposes, and would be available online. I would hope that people who have postcolonial Englishes’ best interests in mind had collected the data, and that there were no ethical bounds crossed in this collection. I also hope that these people were planning or already have done some of their own research on this dataset as that would mean they have a process for altering the data to fit their research question, which might mean it is more tidy than it would be if it was collected independently of any study.\n\n\nAvailable Data\nThe available data being considered will start with ICE Ghana and HiCE Ghana, the two corpora used in “Outdooring the Historical Corpus of English in Ghana”. To get access to ICE Ghana, I would have to show a valid license to use this data, and request access directly from the ICE website. However, I can speak about its format without getting direct access to the data. ICE Corpora are luckily structured in formulaic ways. The written modalities of ICE Corpora are structured as follows: out of 200 written texts, 50 are non-printed and 150 are printed. Of the 50 non-printed texts, 20 are student writing which includes 10 student essays and 10 exam scripts, and 30 are letters, which includes 15 social letters and 15 business letters. Of the 150 printed texts, 40 are academic writing, which includes 10 texts each from humanities, social sciences, natural sciences, and technology, 40 are popular writing, which includes 10 from each of the same subjects, 20 are reportage, namely press news reports, 20 are instructional writing (10 administrative writing, 10 skills/hobbies), 10 are persuasive writing, namely press editorials, and the last 20 are creative writing, which is comprised of novels and short stories. Clearly, ICE Corpora are very well balanced, so this is ideal. HiCE Ghana is also hard to get direct access to, however I have found that it is organized in much the same way as ICE Ghana, simply expanding the years it has data from, and adding more data to all fields. Without direct access to any of these corpora, I cannot tell how well this data is aligned to my ideal data. Other ICE and ICE ajacent corpora will also be considered, including ICE East Africa, which include Kenya and Tanzania, ICE Jamaica, ICE Philippines, ICE Nigeria and ICE Uganda. All will follow the same general format, which will be useful when it comes time to merge these datasets together.\n\n\nResolving Misalignment\nWithout direct access to these datasets at this time, it is impossible to know exactly how to fit the data into something more ideal. However, I can speculate on what it would take to fit the data to my ideal data, given that the ICE Corpora look similar to the data I have encountered in this class so far. Firstly, and perhaps most importantly, the tokenization into words. For any data to be useful to me in this context, it would need to be tokenized, likely by words. If the text was not tokenized in this way, I would need to do so. I would need to be especially careful in this process, because the words or phrases being used in these corpora could be unfamiliar to me, but especially unfamiliar to whatever process is tokenizing the words. If these words are written with dashes, for example, I would need to make sure to keep them together, to preserve meaning, however, this could have other unintended consequences in other circumstances. Once tokenized, the words would need to be retained in order, and probably have a word-id and document-id column, so that they could be reconstructed into a form similar to the original text, if needed. This can be done with a simple R function, and does not pose much of a problem. It would also be important to note the modality of the data, so if that is not already a part of this data, I would attempt to fix that. I imagine, though that this is a part of the ICE corpora, along with the more specific use case of the document the word comes from. I am not certain that the ICE corpora would include information about the writer or speaker, but if this is not included, it would be very difficult to restore this data, as it would not be intertwined with any other data in this dataset. If this is not included, I would have no choice but to do without it. The country of origin would be easy to include, simply in that the corpora are organized this way already. All I would have to do is add a column to each corpus and note the country before merging the corpora together. The year from when the word rose to prominence would be hard to obtain, and would probably be a statistic in and of itself, rather than included in the data. It would be useful to have the year the word was written, then a histogram could be constructed, with the year on one axis and the count of the word on the other, then the most popular year, or the first year where the count of the word crossed a certain threshold could be taken, and then added to the data as another column as ‘year_of_prominence’. In this way, this data could be gleaned. The most difficult column to restore, would be the part of speech. Luckily, many ICE corpora have POS tags, so hopefully this wouldn’t be an issue. If the POS tag was not there, I would have to find a program, likely some sort of AI, to classify these words. This would be largely difficult for this AI, and it may get many words wrong, given that it was probably not trained on post-colonial Englishes. I would have to test its effectiveness in some way, and if its accuracy was below a certain threshold, it would simply not be acceptable to use in this case, and I would have to go without POS tags. Overall, although this is very speculative, I don’t believe that the whole process would be incredibly difficult, as this plan seems to cover many possibilities.\n\n\nAssessing Progress\nIn this project step, I learned to think about what I would actually want in my dataset, even before finding it. Before doing this step, I had imagined that the opposite process was true, adapting a dataset to fit your needs, but really, it helps much more to think about the perfect dataset, and then finding something close to it. Most challenging was the misalignment section, simply because it was very speculative, because I don’t have access to the data I would need. I do have a question on whether or not it is necessary to have direct access to this data to move on with this project. I’m fairly certain I could gain access, it just may take some time. I would need to address this to move forward.\nMy interests have evolved slightly since taking on this project. Originally I wasn’t sure what to do, but I had a few ideas, including the following in my notes: - geographical boundaries of dialect vs. languages - age of borders / national identity vs. sharpness of boundaries of languages - head trauma vs. enunciation of words - foreign word use through history compared to origin language My final bullet point does have something to do with this prospectus, and my interests have kept up with the direction of this project. My research question has remained largely the same, however, I realize how difficult it would be to keep the scope of this prospective study so large. It may be more wieldy to focus the scope on just West Africa, as a more direct logical step from the previous literature. Depending on the ease of obtaining data, the scope would change. Overall, from reading and witnessing in class how much it takes to get POS tags or to do data preparation, even before any analyzing happens, I understand why it is so difficult to do a study on a large scope.",
    "crumbs": [
      "Reports",
      "Prospectus"
    ]
  },
  {
    "objectID": "reports/prospectus.html#data-analysis",
    "href": "reports/prospectus.html#data-analysis",
    "title": "Prospectus",
    "section": "Data analysis",
    "text": "Data analysis\nWhy would we analyze data, though, without purpose? The purpose is as follows: there is a need to extend the current application of existing methods into new areas, making the scope of study larger to represent more Englishes and more people, hopefully without sacrificing too much granularity, as is the worry when expanding the scopes of such studies. The study I am expanding upon, “Outdooring the Historical Corpus of English in Ghana” is a very satisfactory case study on the English of Ghana, but it would be advantageous to expand the study to include more West African Englishes, to increase variation and attempt to find evidence to support or to dissuade a hypothesis.\nAttempting to support or contradict a trend like this would be representative of inferential research. This means we are taking a sample from a population, and all things being equal, the bigger the sample, the better. We are hoping this sample faithfully represents the population so that we can generalize trends present in the sample to the larger population as a whole. My goals are very much similar. I have a question I am asking, “do postcolonial languages develop new words at the same rate, or does the country’s independence either slow or hasten the divergence of postcolonial language?”, and thus I need to either find evidence that supports this conclusion or does not support this conclusion. I hope to find a result in this data, usually called a p-value, that is above a certain threshold, and if it is above this threshold it is not statistically significant, and if it is below this threshold then it is statistically significant. Putting this in statistical terms, this process is called a hypothesis test. A typical p-value is 0.05, and for these purposes I believe it would be acceptable to define our threshold as such.\n\nAnalysis Plan\nIn this research study, it would be advantageous to use simulation-based null hypothesis significance testing, as it is based on empirical data, which we are assuming we have, and does not require any assumptions about the data. This is beneficial because we would rather not assume something that is incorrect and jeopardize the efficacy of the whole study, and would rather simply check what we can about our data. Additionally, it is much easier to communicate later, especially to non-statisticians. To be more specific, I would use a bivariate correlation statistical test, as the explanatory variable: the date of independence, is numerical (number of years) so it can be compared against a numerical response variable: the number of neologisms present in the language as compared to American English, or the english of the colonizing country.\nFinding the number of neologisms, however, is another process that needs to be defined and completed before a hypothesis can be tested. This process would need to compile the words in a corpus and perform some level of frequency analysis on them. This would then be plotted against time, as in the level of usage over time, and if growth is above a certain threshold, as well as having certain other factors, i.e. being a content word, having a higher frequency than its baseline frequency, etc. This process would be largely automated, but still verified to ensure it would be reliable. These neologisms can be counted, year by year to get our number to be tested for correlation against date of independence.\n\n\nAssessing Progress\nI learned how to define a plan for data analysis in practice, and how to execute that plan in theory. I also, frankly learned about hypothesis tests since it had been a while since I had reviewed them in depth. This was likely the most challenging part: defining exactly what I want to do before I do it. I typically take a very hands on approach to programming, and this is simply a more theoretical approach to programming, so this was relatively difficult for me. I do not think I need to address much to move forward, I believe I have a good conceptual grasp on where I am and where I need to be in the coming days. My interests, since last time have evolved only slightly, I have come to the realization that my scope was still too large, so narrowing it down to only encompass North African Englishes seems like a decent compromise between granularity and scope. My research question hasn’t changed since last step, but the way I could directly answer it is now much more clear, due to the slides in class, and the readings outside of it. This step has probably been the most enlightening to me in this respect.",
    "crumbs": [
      "Reports",
      "Prospectus"
    ]
  },
  {
    "objectID": "reports/prospectus.html#brainstorming-presentation",
    "href": "reports/prospectus.html#brainstorming-presentation",
    "title": "Prospectus",
    "section": "Brainstorming presentation",
    "text": "Brainstorming presentation\nThe research question that needs to be answered is whether postcolonial Englishes have their rates of neologisms affected by the time since independence. This can be answered by talking about my potential data preparation and what I would do with that data if I had it. I can also talk about how I would analyze that data with a hypothesis test. The significance of this research could be an exploration into linguistic evolution between English and various indigenous languages. If the difference between countries is great, even if they have similar dates of independence, there may be more complex cultural dynamics at play. Additionally, the rates of neologisms could play a part in the choices of these governments to set up policy for conserving indigenous languages. Some takeaways for the audience could include that this research is valuable to those speaking these Englishes, and that these languages are just as important, if not more important that more standard Englishes.\nMy audience is likely going to be academics, but not those that are experts on this subject. This means that I will have to very plainly explain what I am doing, and why I am doing it. Additionally, because I will not actually have access to data, and thus I will not have a concrete answer to my question, I will have to make sure the audience is actually interested in what I am presenting. My audience will likely have an interest in this subject, so that won’t be too difficult, but I will try to make a relatively short and engaging presentation. Part of that can be not directly reading off the slides, but having short blurbs on the screen, and being able to branch off of those during the presentation itself. I want to make it clear that I want feedback from the audience, as they have a similar education level to myself, so there is a hope that we can put our heads together, and help make this prospectus even more valuable.\nThe presentation outline can go as follows: first, a literature review section, talking about Ghanian English Nativization. I want to note the main findings of the study, and why it is important to my case, as well as the research gap that I had found in this study. After this, I can move onto my research question. I can talk about how I came up with this question, and where it came from, and then list the actual question in full. I can then talk about the data preparation. This section may be short because I don’t actually have data, but I can talk about where I could get it from, and some things about the potentially available data, as well as the ideal data I would like. Finally, I can talk about how I would analyze this data: looking for correlation between the explanatory variable of the time since independence versus the dependent variable of the rates of neologisms. I can talk about which test I would use, as I am looking for correlation. I can also quickly describe what type of research this is: exploratory, inferential or predictive. I will probably include pictures, just to make sure the reader’s attention is at least somewhat on the screen, but they are not the focus of this presentation.",
    "crumbs": [
      "Reports",
      "Prospectus"
    ]
  },
  {
    "objectID": "reports/prospectus.html#brainstorming-prospectus-revision",
    "href": "reports/prospectus.html#brainstorming-prospectus-revision",
    "title": "Prospectus",
    "section": "Brainstorming prospectus revision",
    "text": "Brainstorming prospectus revision\nI didn’t get too much feedback from my peers, mostly just clarifying questions, but the instructor gave me some feedback; he talked about how finding neologisms and comparing their rates to other languages may by too much, along with the increased scope. I believe the takeaway here is to avoid losing too much granularity with the increase in scope. Overall, I will adjust some of the wording in my prospectus, perhaps, but there is not much I can do at this point to address this issue. My whole prospectus is based around this feature.\nI need to revise some words here and there, but this is a hard bit of feedback to integrate. I think I will mention it in my conclusion as a potential speed bump to actually enacting this prospectus, and mention it where I can. The scope of my research is clear, although it may be too broad to enact effectively. I believe though, that it is possible to do in practice. The hardest part is getting the data, and perhaps defining the neologisms. This is a far from impossible task though, and with the right resources, it can be done. My research question, data preparation strategy and analysis plan seem to be well aligned, and are definitely well-articulated; I have large sections of this prospectus describing them in detail. I surely have sufficient detail to guide my research. The only hiccup is getting the data.\nTo complete my research plan, I still need to include a short paragraph in a few different sections: the Conclusion section, References (will include the studies mentioned in the lit review), the Appendix (although I will probably take this out considering I have nothing to put there) and the Introduction section. After this and a final check-over, I imagine my prospectus should be ready to finalize and push to github.\nDevelop a plan for finalizing your research plan (prospectus). What steps do you need to take to complete your research plan? What resources or support do you need to finalize your research plan?",
    "crumbs": [
      "Reports",
      "Prospectus"
    ]
  },
  {
    "objectID": "reports/article.html#data",
    "href": "reports/article.html#data",
    "title": "Article",
    "section": "Data",
    "text": "Data",
    "crumbs": [
      "Reports",
      "Article"
    ]
  },
  {
    "objectID": "reports/article.html#analysis",
    "href": "reports/article.html#analysis",
    "title": "Article",
    "section": "Analysis",
    "text": "Analysis",
    "crumbs": [
      "Reports",
      "Article"
    ]
  },
  {
    "objectID": "reports/presentations.html",
    "href": "reports/presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Slide deck\n\n\n\n\n\nJan 1, 2021\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Reports",
      "Presentations"
    ]
  },
  {
    "objectID": "reports/presentations.html#latest-presentations",
    "href": "reports/presentations.html#latest-presentations",
    "title": "Presentations",
    "section": "",
    "text": "Slide deck\n\n\n\n\n\nJan 1, 2021\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Reports",
      "Presentations"
    ]
  }
]